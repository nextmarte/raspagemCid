---
title: "Como Raspar dados CID Lagoa Viva"
subtitle: |
        Técnicas e métodos para coleta de dados <br> revisão da literatura e estudo de caso
author: "Marcus Ramalho e Ariel Levy"
date: today
logo: "https://lagoaviva.net/pluginfile.php/1/theme_mb2nl/logo/1707142444/logo-lagoa-viva-v3.png"
toc: false
bibliography: references.bib
csl: apa.csl
incremental: true
---

## O que é web scraping?

<center>
```{mermaid}

graph LR
    A("fa:fa-globe")
    B("fa:fa-laptop-code")
    C["fa:fa-database"]
    D["fa:fa-file-csv"]
    E["fa:fa-code json"]
    F["fa:fa-file-excel"]
    G["fa:fa-code xml"]
    H["fa:fa-code html"]
    A --> B
    B --> D
    B --> E
    B --> F
    B --> G
    B --> H
    D --> C
    E --> C
    F --> C
    G --> C
    H --> C



```

</center>


## Design básico de um web scraper

![Fonte: [@lotfi2021]](images/paste-1.png){fig-align="center" width="578"}

![](images/paste-2.png){fig-align="center" width="493"}

## Métodos: DOM parsing (Document Object Model)

<center>

```{mermaid}
graph TB
    A[HTML]
    B[HEAD]
    C[BODY]
    D[DIV class='card']
    E[Text]
    F[DIV class='card-body']
    G[Text]
    H[H5 class='card-title']
    I[Text: Lagoa Viva]
    J[Text]
    K[P class='card-text']
    L[Text: A Plataforma ...]
    M[Text]
    N[Text]
    A --> B
    A --> C
    C --> D
    D --> E
    D --> F
    F --> G
    F --> H
    H --> I
    F --> J
    F --> K
    K --> L
    K --> M
    K --> N
```

</center>

Organiza o documento em uma estrutura de árvore - DOM: <https://livedom.validator.nu/>


## Requisitos para raspar dados de um site (html parsing)

Conhecimento básico de HTML e CSS (para identificar os elementos que queremos coletar), tags e atributos.



<details>

<summary>Exemplo</summary>

```{.html code-line-numbers="1|2|3|4"}
<div class="card">
    <div class="card-body">
        <h5 class="card-title">Lagoa Viva</h5>
        <p class="card-text">A Plataforma Educacional Lagoa Viva possui uma grande inovação para a população de Maricá: o sistema de avaliação e identificação de perfis comportamentais. Vamos entender como isso funciona?</p>
    </div>
</div>
```

</details>

Para coletar o texto do título e do parágrafo do card precisamos identificar os elementos que contém esses textos. No caso, o título está dentro de uma tag `h5` e o parágrafo dentro de uma tag `p`.

<details>

<summary>Código em R para coletar esses dados:</summary>

```{r}
#| eval: false
#| echo: true
#| code-line-numbers: "1|2|3|4|5|6|7|8|9|10|11"
library(rvest)
html <- "html do card"
webpage <- read_html(html)
title <- webpage %>%
    html_node("h5.card-title") %>%
    html_text(trim = TRUE)
text <- webpage %>%
    html_node("p.card-text") %>%
    html_text(trim = TRUE)

```

</details>

<details>

<summary>Código em Python para coletar esses dados:</summary>

```{.python code-line-numbers="1|3|4|"}
from bs4 import BeautifulSoup
html = "html do card"
soup = BeautifulSoup(html, 'html.parser')
title = soup.find('h5', class_='card-title').text
text = soup.find('p', class_='card-text').text

```

</details>



## Métodos: Regular Expression (Regex) e LLM's (Large Language Models)

Regular expressions são padrões utilizados para encontrar determinadas combinações de caracteres dentro de strings. Exemplo: regex: `r'\d+'` encontra todas as sequências de números em uma string.

<center>

![](images/paste-6.png)

</center>

Modelos de linguagem aprendem a estrutura e padrões de uma língua natural. Exemplo: Gemini, GPT-4, etc... Uma aplicação de LLM's é a extração de informações de textos, como entidades nomeadas, relações ou estruturas de páginas web.

## Outros métodos

- XPath (XML Path Language)
- Vertical Aggregation Platforms (VAP)
- Semantic annotation recognizing
- Computer Vision Web Page Analyzer

## API`s

- Segurança e confiabilidade
- Facilidade de uso
- Documentação
- Limites de requisições e preços
- Autenticação
- Pacotes Python: `requests`(Faz requisições HTTP e HTTPS. ) e `asyncio`(Permite requisições assíncronas.)
- Pacotes R equivalentes: `httr` e `async`


## Web Crawling

- Coleta de dados de múltiplas páginas de um site
- Torna o processo de coleta de dados mais eficiente 
- Ferramentas: Scrapy, Selenium, Puppeteer, etc...
- Possíveis problemas: bloqueio de IP, captchas, etc...

## Pipeline de coleta de dados

- Coleta de dados: web scraping, web crawling, API's, etc...
- Limpeza (gargalo do processo) 
  - Python: `pandas`, `re`
  - R: `rvest`, `stringr`, `tidyverse`, 
- Armazenamento
- Para raspagens rápidas e simples: R 
- Para raspagens mais complexas e crawlers: Python


## Referências