---
title: "Como Raspar dados CID Lagoa Viva"
subtitle: |
        Técnicas e métodos para coleta de dados <br> revisão da literatura e estudo de caso
author: "Marcus Ramalho e Ariel Levy"
date: today
logo: "https://lagoaviva.net/pluginfile.php/1/theme_mb2nl/logo/1707142444/logo-lagoa-viva-v3.png"
toc: false
bibliography: references.bib
csl: apa.csl
incremental: true
---

## O que é web scraping?

<center>
```{mermaid}
graph LR
    A{Internet}
    B(software/código)
    C[database]
    D[csv]
    E[json]
    F[xlsx]
    G[xml]
    A --> B
    B --> C
    B --> D
    B --> E
    B --> F
    B --> G

```

</center>


<!-- 
![Fonte: [@webharvyWhatWebScraping]](images/paste-3.png){fig-align="center"} -->

## Design básico de um web scraper

![Fonte: [@lotfi2021]](images/paste-1.png){fig-align="center" width="578"}

![](images/paste-2.png){fig-align="center" width="493"}

## Requisitos para raspar dados de um site (html parsing)

Conhecimento básico de HTML e CSS (para identificar os elementos que queremos coletar), tags e atributos.



<details>

<summary>Exemplo</summary>

``` html
<div class="card">
    <div class="card-body">
        <h5 class="card-title">Lagoa Viva</h5>
        <p class="card-text">A Plataforma Educacional Lagoa Viva possui uma grande inovação para a população de Maricá: o sistema de avaliação e identificação de perfis comportamentais. Vamos entender como isso funciona?</p>
    </div>
</div>
```

</details>

Para coletar o texto do título e do parágrafo do card precisamos identificar os elementos que contém esses textos. No caso, o título está dentro de uma tag `h5` e o parágrafo dentro de uma tag `p`.

<details>

<summary>Código em R para coletar esses dados:</summary>

```{r}
#| eval: false
#| echo: true
#| code-line-numbers: "1|2|3|4|5|6|7|8|9|10|11"
library(rvest)
html <- "html do card"
webpage <- read_html(html)
title <- webpage %>%
    html_node("h5.card-title") %>%
    html_text(trim = TRUE)
text <- webpage %>%
    html_node("p.card-text") %>%
    html_text(trim = TRUE)
print(title)
print(text)
```

</details>

<details>

<summary>Código em Python para coletar esses dados:</summary>

```{.python code-line-numbers="1|2|3|4|5|6|7"}
from bs4 import BeautifulSoup
html = "html do card"
soup = BeautifulSoup(html, 'html.parser')
title = soup.find('h5', class_='card-title').text
text = soup.find('p', class_='card-text').text
print(title)
print(text)
```

</details>


## Métodos: DOM parsing (Document Object Model)

<center>

```{mermaid}
graph TB
    A[HTML]
    B[HEAD]
    C[BODY]
    D[DIV class='card']
    E[Text]
    F[DIV class='card-body']
    G[Text]
    H[H5 class='card-title']
    I[Text: Lagoa Viva]
    J[Text]
    K[P class='card-text']
    L[Text: A Plataforma ...]
    M[Text]
    N[Text]
    A --> B
    A --> C
    C --> D
    D --> E
    D --> F
    F --> G
    F --> H
    H --> I
    F --> J
    F --> K
    K --> L
    K --> M
    K --> N
```

</center>

Organiza o documento em uma estrutura de árvore - DOM: <https://livedom.validator.nu/>

## Métodos: Regular Expression (Regex) e LLM's (Large Language Models)

Regular expressions são padrões utilizados para encontrar determinadas combinações de caracteres dentro de strings. Exemplo: regex: `r'\d+'` encontra todas as sequências de números em uma string.

<center>

![](images/paste-6.png)

</center>

Modelos de linguagem aprendem a estrutura e padrões de uma língua natural. Exemplo: Gemini, GPT-4, etc... Uma aplicação de LLM's é a extração de informações de textos, como entidades nomeadas, relações ou estruturas de páginas web.

## Outros métodos

- XPath (XML Path Language)
- Vertical Aggregation Platforms (VAP)
- Semantic annotation recognizing
- Computer Vision Web Page Analyzer

## API`s

- Segurança e confiabilidade
- Facilidade de uso
- Documentação
- Limites de requisições e preços
- Autenticação
- Pacotes Python: `requests`(Faz requisições HTTP e HTTPS. ) e `asyncio`(Permite requisições assíncronas.)
- Pacotes R equivalentes: `httr` e `async`


## Aplicações de web scraping com Python e R



## Referências